### Academic statement

My main interest in neuroscience is the problem of learning and memory: how neural systems of varying degrees of complexity can store and retrieve information accurately and reliably, in spite of the formidable constraints imposed on them by biology. Memory is an integral part of all neural systems capable of adapting to their environments, and a necessary process for the scientific community to understand if we want to move beyond the simplistic model of the brain as a mapping from perceptual inputs to behavioral outputs. My goal in research, at least for the near future, is to apply my background in computer science and statistics to the study of memory, in order to develop both tools for analyzing experimental data and models of the computations performed by neural circuits involved in storage and retrieval of memories.

My first exposure to neuroscience was at the beginning of my senior year at Princeton, when I started work on my thesis project with Prof. Jonathan Pillow. One of the problems that Prof. Pillow was working on at the time was figuring out how to fit certain classes of statistical models with extremely complicated, difficult-to-compute likelihood functions to neural data. Because these likelihood functions were effectively noisy black boxes, traditional optimization methods didn't work well on them, and Prof. Pillow had turned to an algorithm called Bayesian optimization, which recasts the problem of black-box optimization as one of Bayesian inference: the function is assumed to be drawn from some prior stochastic process, which, when combined with observations of the function's values at a set of chosen test points, yields a posterior distribution over possible functions, and an expected optimum. The thing that makes Bayesian optimization effective is that it is *iterative*: each time the function is evaluated at a test point, we re-compute the posterior distribution over possible functions and use it to choose the next test point to evaluate at. Many different strategies exist for choosing test points, but none of them dealt adequately with the types of functions Prof. Pillow was trying to optimize, where the level of noise in an observation was dependent on the input value. For my thesis project, I developed a method for performing Bayesian optimization on these types of functions, inspired by a result from the experimental design literature, and demonstrated it using numerical simulations. For the summer after my senior year, I continued working in Prof. Pillow's lab as a research assistant, applying my algorithm to more general classes of functions. Although the project I worked on in Prof. Pillow's lab was only tangentially related to neuroscience, I learned about the field by attending our lab meetings and discussing papers, and I became familiar with many of the models and tools used for analyzing neural data.

In May of this year, after about a year and a half of working as a software engineer at Bloomberg L.P. in New York, I joined Prof. Cristina Savin's lab at NYU as a research assistant. Prof. Savin's main research focus is computational modeling of memory, plasticity and learning, which lines up perfectly with my interests. So far, I have worked primarily on using Gaussian process models (the same type of models I used for my Bayesian optimization work with Prof. Pillow) to infer tuning curves from neural recordings. For the first project I worked on, starting in June, we tried to apply this approach to calcium imaging data from one of our collaborators at NYU Medical School, which was recorded from the CA3 hippocampal region in rodents performing a spatial navigation task. Unfortunately, this data turned out to be too sparse for our tuning curve inference to work on it. This was disappointing for sure, but also gave me valuable experience working with real-world experimental data, which I have really come to enjoy. For the second project, which we are still working on, we are applying the same method to electrophysiological data from one of our collaborators at NYU, recorded from the medial entorhinal cortex of rodents performing a different spatial navigation task. Luckily, we've been able to reuse a lot of the data analysis tools we developed for the last project on this one.

One of the things that I've really liked about this lab is the emphasis Prof. Savin places on biological realism. As someone coming to neuroscience from the more theoretical side, it's easy to lose sight of how important it is to get the biological details right when we try to build computational models of the brain. Ultimately, it's the mathematical contraints that arise from this biological reality that make the questions we are asking complex and interesting in the first place. One of the things that I really hope to get out of pursuing a PhD is the opportunity to learn more about the biological side of neuroscience, in both my classes and lab rotations.

One of the things that I really like about the research being done at NYU's Center for Neural Science is the emphasis placed on biological realism, even in the more computationally-oriented labs. As someone coming to neuroscience from the more theoretical side, it can be easy to lose sight of how important it is to get the biological details right when we build computational models of the brain. Ultimately, it's the mathematical contraints that arise from this biological reality that make the computational questions we are asking complex and interesting in the first place. Over the past few months I've been working here, I've seen this theoretically principled, biologically conscious approach to neuroscience in Prof. Savin's work on memory and learning, as well as Prof. Eero Simoncelli's work on vision, and Prof. Wei Ji Ma's work applying Bayesian inference to decision-making. Because of the opportunity to work with researchers like these, as well as the opportunity to engage seriously with the biology of neural systems while still advancing my knowledge of theoretical principles, I believe NYU would be an excellent place to pursue my PhD in neuroscience.
