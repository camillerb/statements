### Academic statement

My main interest in neuroscience is the problem of learning and memory: how neural circuits of varying degrees of complexity can store and retrieve information accurately and reliably, in spite of the formidable constraints imposed on them by biology. This is a necessary process for the scientific community to understand if we want to move beyond a simplistic model of the brain as a mapping from perceptual inputs to behavioral outputs. My goal in research, at least for the near future, is to apply my background in computer science and statistics to the study of memory, in order to develop both tools for analyzing experimental data and models of the computations that neural circuits perform in order to store and retrieve memories.

My first exposure to neuroscience research was during my senior year at Princeton, when I worked on my thesis project with Prof. Jonathan Pillow. One of the problems that Prof. Pillow was working on at the time was figuring out how to fit statistical models with complicated likelihood functions to neural data. Because these likelihood functions were effectively noisy black boxes, traditional optimization methods didn't work well on them. Instead, Prof. Pillow was using an approach called Bayesian optimization, which recasts the problem of optimization as one of Bayesian inference: the function being optimized is assumed to be drawn from some prior stochastic process, which, when combined with observations of the function's values at a set of chosen test points, yields a posterior distribution over possible functions, and an expected optimum. The thing that makes Bayesian optimization effective is that it is *iterative*: each time the function is evaluated at a test point, we re-compute the posterior distribution over possible functions and use it to choose the next test point to evaluate at. Many different strategies exist for choosing test points, but none of them dealt adequately with the types of functions Prof. Pillow was trying to optimize, where the level of noise in a function observation was not constant, but dependent on the input value. For my thesis project, I developed a method for performing Bayesian optimization on these types of functions, inspired by a result from the experimental design literature, and demonstrated it using numerical simulations. For the summer after my senior year, I continued working in Prof. Pillow's lab as a research assistant, applying my algorithm to more general classes of functions. Although the project I worked on in Prof. Pillow's lab was only tangentially related to neuroscience, I learned about the field by attending our lab meetings and discussing papers, and I became familiar with many of the models and tools used for analyzing neural data.

In May of this year, after a year and a half of working as a software engineer at Bloomberg L.P. in New York, I joined Prof. Cristina Savin's lab at NYU as a research assistant. Prof. Savin's research focus is computational modeling of memory, plasticity and learning, which lines up really well with my current interests. So far, I have worked primarily on using Gaussian process models (the same type of models I used for my Bayesian optimization work with Prof. Pillow) to infer tuning curves from neural recordings. While Gaussian process models have existed for a long time, algorithms that tractably fit them to large datasets have only recently been introduced, allowing us for the first time to use them to analyze many types of neural data. For the first project I worked on, starting in June, we tried to apply this approach to calcium imaging data recorded from the CA3 hippocampal region in rodents performing a spatial navigation task. Unfortunately, this data turned out to be too sparse for our tuning curve inference to work on it. This was disappointing for sure, but also gave me a lot of valuable experience working with experimental data. For the second project, which we are still working on, we are applying the same method to electrophysiological data recorded from the medial entorhinal cortex of rodents performing a different spatial navigation task. Luckily, we've been able to reuse a lot of the data analysis tools we developed for the first project on this one.

One of the things that I really like about the research being done at NYU's Center for Neural Science is the emphasis placed on biological realism, even in the more computationally-oriented labs. As someone coming to neuroscience from the more theoretical side, it can be easy to lose sight of how important it is to get the biological details right when we build computational models of the brain. Ultimately, it's the mathematical contraints that arise from this biological reality that make the computational questions we are asking complex and interesting in the first place. Over the past few months I've been working here, I've seen this theoretically principled, biologically conscious approach to neuroscience in Prof. Savin's work on memory and learning, as well as Prof. Eero Simoncelli's work on vision, and Prof. Wei Ji Ma's work applying Bayesian inference to decision-making. Because of the opportunity to work with researchers like these, as well as the opportunity to engage seriously with the biology of neural systems while still advancing my knowledge of theoretical principles, I believe NYU would be an excellent place to pursue my PhD in neuroscience.
